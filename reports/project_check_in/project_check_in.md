Group 9
Professor Simmons, ECS 170
November 12th, 2025
Project Check-In

Our original proposal focused on training a reinforcement learning agent capable of playing Pac-Man using Q learning and Deep Q learning. Since then, we decided to use Ms. Pac-Man as the primary environment instead of Pac-Man. This decision was motivated by Ms. Pac-Man’s complex gameplay with multiple changing mazes and randomization of fruits and ghosts' movements. This less predictable variation allows for a richer environment for our agents to demonstrate intentional and meaningful learning rather than memorizing a fixed path. 
While our overall primary goal remains the same, we have shifted our technical emphasis from simple Q learning to Deep Q-Networks due to the environment’s high-dimensional state space. This project now leans heavily on deep learning integration and function approximation. As per the feedback, we also dropped BFS as an approach entirely, because Ms.Pac-Man is super non-deterministic. Thus, BFS would not be effective for the Pac-Man agent due to the randomness of the ghost behavior. After further research, we expanded our scope to include two more agent algorithms, PPO and Microsoft HRA, which were both used in the past to achieve scores outperforming humans.

Our team has completed several key milestones:
- Environment Setup: We successfully configured the Atari Learning Environment (ALE) and Gymnasium frameworks to support Ms. Pac-Man. The Python virtual environment (venv) and installed dependencies from requirement.txt to ensure reproducibility
- Repository Setup: We established our project repository on GitHub and included dedicated directories for agents, configurations, model checkpoints, and results.
- Base Agent Development: A key milestone was the creation of the Base Agent. This file serves as the template for all other agents (Q learning, DQN, PPO) in our project, enforcing a consistent interface. This structure allows us to implement new reinforcement learning models without rewriting the interaction code.
- Implemented Agents: We have developed or are currently testing the following agents
	- Random Agent: The first test case used to verify that the environment and training loops functioned properly. 
	- Q‑Learning Agent: Implemented a tabular Q‑learning baseline on Ms. Pac‑Man’s RAM. It quickly picked up short‑term habits like eating pellets, basic ghost avoidance, confirming the environment and the reward pipelines work.
	- DQL Agent: Introduces a neural network to approximate Q values, giving the agents the ability to handle larger state spaces
	- DQN Learning Agent: Expands on DQL by adding a target network, replay memory, and gradual exploration decay to make training more stable and efficient.
	- PPO Agent: PPO (Proximal Policy Optimization) is a policy-based method that computes probabilities to derive a policy, and learns the policy directly. It functions with an Actor-Critic architecture. After the convolutional layer processes the Atari grid, the actor network spits out probabilities for each possible action, and then improves smoothly by exploring and learning different strategies with different probabilities. The critic network just judges how good the current state of Ms. Pacman is, and outputs the expected value of points the agent should get from a specific position. To find out which action was actually good or bad, we implemented a generalized advantage estimation (GAE) to look at future rewards and using the critic network predictions to guess from. PPO’s advantages include no need for replay buffers or target networks, better exploration, and more stability with clipped rewards. In addition, PPO has been shown to be just as good, and even better than Q learning methods.
	- Microsoft HRA Agent: Finally, we also trained a Microsoft HRA (Hybrid Reward Architecture) agent. This specific algorithm was the first to ever achieve a perfect score in Ms. Pac-Man using a “divide and conquer” method. It divides rewards across multiple sub-agents to optimize objectives simultaneously.

One major challenge was the slow training on the CPU due to high variability in the game environment. The randomness in ghost movement and maze transitions increased the variance in our results. To address this, we utilized a preprocessing environment that made the results of the training much more consistent. In addition, we used cloud compute environments, like Google Colab, to train our models on GPUs with more resources instead of CPUs. Another issue was the lack of observability with our agent performance, as we didn’t properly set up metrics and logging, which means the only metric/evaluation we have is the change in rewards over time (episodes played). We are now adding logging and visualization tools to track losses, exploration rates, and reward distributions over time. 
For the PPO agent, handling rewards was a challenge. We ran into lots of instability while training due to the Atari rewards, specifically for Pac-Man, varying rapidly. Ms. Pac-Man’s rewards varied from 10-point pellets to 100-point blue ghosts leading to huge gradients and high-risk of overfitting rare-reward events. As we didn’t want to compromise our learning rate, rather than reducing the learning rate, we instead clipped the rewards to focus on bad, neutral, and good rewards with a space of [-1, 0, 1]. The tradeoff is information loss, with limited hierarchy and strategy, but this also cuts on the time spent tuning hyperparameters and training with episodes. Going forward, we plan to add adaptive normalization to the mix (based on this paper) so that we can still focus on stability without information loss.
Another issue that came up was the variability of the environment. The games had sprites that flickered on and off due to computational load, which caused some entities to become invisible for individual frames. The Atari grid was also too detailed with extremely high resolutions and unnecessary colours which passed too much information into our CNN. This was resolved with a preprocessing wrapper that scaled the resolution down to 84 x 84 pixels and made all RGB channels grayscale. Additionally, we added a frame skip of 4 allowing agents to react 15 times per second compared to the original 60, hashing out redundancy and skill capturing key motions of entities in the environment.
We were surprised by how sensitive our agents were to small changes in parameters. Tiny adjustments to rewards and learning rate could completely alter behavior. We also didn’t expect how fast the Double Q-Learning agent would outperform the standard DQN; it was smoother and had fewer random failures. Sometimes, even the random agent would survive longer than the learning models, showing how unpredictable reinforcement learning can be. 
Since the proposal, we have expanded our approach from using only Q-learning to implementing Deep Q-learning and Deep Q Network using neural networks to approximate action value functions more efficiently. We also added a PPO agent for policy-based reinforcement comparison and a Microsoft HRA Agent to test heuristic strategies. These new updates allow us to create new learning pathways for the same Pac-Man environment and better analyze which one yields the most stable and adaptive gameplay behavior.
As we move forward, some potential roadblocks will likely be computational limits and training convergence. Long episodes are necessary for consistent outcomes in reinforcement learning, and if the parameters are not properly adjusted, DQN models may still diverge. To combat this, we plan to checkpoint models every 100 episodes, keeping an eye on reward curves for early indication of instability and modify learning rates if needed. We also anticipate difficulty integrating visualization outputs for the presentation, so we will test rendering tools to record gameplay clips efficiently.
Preliminary tests show that Double Q-Learning and PPO agents are learning more stable strategies than simple Q learning. The DQN model demonstrates early gains in average rewards and survival time. These findings imply that our strategy is headed in the correct direction since our agents are successfully modifying their gameplay behavior through reinforcement and engagement rather than memory. 

PPO Agent Rewards (Clipped) Over Time
![PPO Agent Rewards (Clipped) Over Time](./ppo_rewards_latest.png)

The moving average (orange) is an average taken over the period of 100 episodes, since the raw episode reward was too variable to deduce a clear trendline from.
Reflection: With exploring multiple approaches – Q-learning, DQN, PPO, and HRA – we gained a deeper understanding of the strengths and weaknesses of different RL paradigms. We saw how tabular methods break down in high-dimensional spaces, how neural networks introduce new challenges, and why policy-based methods like PPO often behave more smoothly. Overall, this project strengthened our intuition about how AI agents learn in dynamic environments and gave us a practical appreciation for both the potential and the limits of reinforcement learning.
Questions and Requests for feedback: We would appreciate feedback on whether our current evaluations, mainly episode rewards and survival time, are enough or if we should track additional metrics. In addition, any suggestions on how to best visualize our results for the final presentation would be helpful. 

Group Contribution & Work Division: All members have contributed an equal amount of effort into the project. Each agent was split amongst the members, and members who worked less on coding focused on the writing aspect of the project. For the remaining parts of the project, we plan to divide the work evenly within our group to create the charts, metrics, and visualizations we still need, along with learning curves that will show our model’s training progress, score distributions, and demo game videos.

